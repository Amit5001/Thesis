{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "In the non linear case our model is given by:\n",
    "\\begin{gather}\n",
    "\\dot{x} = f(x) + g(x) \\cdot u \\\\\n",
    "x \\in \\mathbb{R}^n , u \\in \\mathbb{R}^m\n",
    "\\end{gather}\n",
    "\n",
    "The optimal controller is calculated by solving the following optimization problem:\n",
    "\\begin{gather}\n",
    "J(u^*) = \\min_{u} \\int_{0}^{\\infty} x^T Q x + u^T R u dt \\\\\n",
    "\\text{subject to:} \\\\\n",
    "h(0) = 0 , R > 0 , Q > 0\n",
    "\\end{gather}\n",
    "\n",
    "And we know that the optimal controller is given by:\n",
    "\\begin{gather}\n",
    "u^* = -R^{-1} g^T(x) \\cdot \\nabla V(x)\n",
    "\\end{gather}\n",
    "\n",
    "Where V(x) is the value function of the system and is given by:\n",
    "\\begin{gather}\n",
    "\\nabla V^T f(x) + h^T(x)\\cdot h(x) - \\frac{1}{4} \\nabla V(x) g(x) R^{-1} g^T(x) \\nabla V(x) = 0\n",
    "\\end{gather}"
   ],
   "id": "dbc02e977054e508"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Based on Kleinman algorithm we can write the equations for the non linear case:\n",
    "\\begin{gather}\n",
    "V_i (x) = \\int_{t}^{\\infty} (x^T Q x + u_i^T R u_i) dt \\\\\n",
    "\\dot{V_i} = \\nabla V_i^T(x)\\cdot \\dot x = -x^T Q x - u_i^T R u_i \\\\\n",
    "\n",
    "\\end{gather}\n",
    "\n",
    "If we substitude in the $V_i^T(x) \\cdot \\dot x $ the model equation we will receive $\\textcolor{red}{\\text{Bellman Equation:}}$:\n",
    "\n",
    "\\begin{gather}\n",
    "\\nabla V_i^T \\left[ f(x) + g(x) u_i \\right] = -x^T Q x - u_i^T R u_i \\\\\n",
    "\\downarrow \\\\\n",
    "\\nabla V_i^T \\left[ f(x) + g(x) u_i \\right] + x^T Q x + u_i^T R u_i = 0\\\\\n",
    "\\end{gather}\n",
    "\n",
    "The equation above also called $\\textcolor{red}{\\text{Policy Evaluation}}$.\n",
    "\n",
    "\n",
    "For a given $u_i$, we can calculate $V_i(x)$ with the initial condition $V_i(0)=0$\n",
    "\n",
    "The Policy evaluation equation is analagous to the Lyapunov equation from the linear case."
   ],
   "id": "a4322f2d42b1d5fa"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Based the new $V_i(x)$ calculated from the Policy evaluation equation we can calculate the optimal controller using the relation of $V_i(x)$ and $u_i$:\n",
    "\n",
    "\\begin{gather}\n",
    "u_{i+1}(x) = -\\frac{1}{2}\\cdot R^{-1} g^T(x) \\cdot \\nabla V_i(x)\n",
    "\\end{gather}\n",
    "\n",
    "The equation above is called as $\\textcolor{red}{\\text{Policy Improvement}}$.\n",
    "\n",
    "This equation is analagous to the linear calculation of the optimal controller:\n",
    "\n",
    "\\begin{gather}\n",
    "K_{i+1} = -R^{-1} B^T P_i\n",
    "\\end{gather}\n",
    "\n",
    "Where $P_i$ is the solution of the Lyapunov equation."
   ],
   "id": "ff04d546b631a7db"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The Data based algorithm we will use here is called Off- Policy reinforcement learning. \n",
    "\n",
    "Thats because that the data we will use to update the policy didnt obtain from the previous policy."
   ],
   "id": "2b1ccc9193de656e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Deriving the new model:\n",
    "\n",
    "We will write the model as we did in the linear case:\n",
    "\n",
    "\\begin{gather}\n",
    "\\dot{x} = f(x) + g(x) \\cdot u_i +  \\\\\n",
    "x \\in \\mathbb{R}^n , u \\in \\mathbb{R}^m\n",
    "\\end{gather}\n",
    "\n",
    "Taking The derivative of the value function $V_i(x)$ with respect to time we will have:\n",
    "\n",
    "\\begin{gather}\n",
    "\\dot{V_i} = \\nabla V_i^T(x)\\cdot \\dot x = -x^T Q x - u_i^T R u_i \\\\\n",
    "\\nabla V_i^T(x) \\cdot \\dot x= \\nabla V_i^T(x) \\left[f(x) + g(x) u_i \\right] + \\nabla V_i^T(x) \\cdot g(x) \\left[u - u_i \\right]\\\\\n",
    "\n",
    "\\end{gather}\n",
    "\n",
    "We can use Bellman equation to replace $f(x) and g(x)$ in the equation above:\n",
    "\n",
    "\\begin{gather}\n",
    "\\nabla V_i^T(x) \\dot x = \\nabla V_i^T(x) \\left[f(x) + g(x) u_i \\right] + \\nabla V_i^T(x) \\cdot g(x) \\left[u - u_i \\right] = -x^T Q x - u_i^T R u_i +  \\nabla V_i^T(x) \\cdot g(x) \\left[u - u_i \\right]\\\\\n",
    "\\end{gather}\n",
    "\n",
    "Using the policy improvement equation we can write:\n",
    "\n",
    "\\begin{gather}\n",
    "\\dot V(x(t)) = \\nabla V_i^T(x) \\dot x = -x^T Q x - u_i^T R u_i - 2 u_{i+1}^T(x) R \\left[u-u_i \\right] \\\\\n",
    "\\end{gather}\n",
    "\n",
    "## Integrating both sides:\n",
    "\n",
    "\\begin{gather}\n",
    "\\int_{t}^{t+T} \\dot V_i(x(t)) dt = - \\int_{t}^{t+T} x^T Q x dt - \\int_{t}^{t+T} u_i^T R u_i dt - 2 \\int_{t}^{t+T} u_{i+1}^T(x) R \\left[u-u_i \\right] dt \\\\\n",
    "\\downarrow \\\\\n",
    "V_i(x(t+T)) - V_i(x(t)) = - \\int_{t}^{t+T} x^T Q x dt - \\int_{t}^{t+T} u_i^T R u_i dt - 2 \\int_{t}^{t+T} u_{i+1}^T(x) R \\left[u-u_i \\right] dt \\\\\n",
    "\\end{gather}\n",
    "\n"
   ],
   "id": "f5b8f9e77362a5be"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "In the linear case we knew that $V_i(x) = x^T P_i x$. In the non linear case we dont know the form of $V_i(x)$.\n",
    "\n",
    "A neural network can be an approximation of a non linear function, we will use the following approximations:\n",
    "\n",
    "\\begin{gather}\n",
    "\\hat V_i(x) = \\sum_{j=1}^{N_1} \\hat c_{i,j} \\phi_j(x) = \\hat C_i \\phi(x) \\\\\n",
    "\\hat u_i(x) = \\sum_{j=1}^{N_2} \\hat w_{i,j} \\psi_j(x) = \\hat W_i \\psi(x) \\\\\n",
    "\\end{gather}\n",
    "    \n",
    "\n",
    "$\\phi(x)$ and $\\psi(x)$ are the basis functions of the neural network.\n",
    "\n",
    "$\\hat C_i$ and $\\hat W_i$ are the weights of the neural network.\n",
    "    \n",
    "Substituting the approximations in the equation above we will have:\n",
    "\n",
    "\\begin{gather}\n",
    "\\sum_{j=1}^{N_1} \\hat c_{i,j} \\left[\\phi_j(x(t+T)) - \\phi_j(x(t)) \\right]= - \\int_{t}^{t+T} x^T Q x +u_i^T \\cdot R \\cdot u_idt - \\int_{t}^{t+T} \\sum_{j=1}^{N_2} \\hat w_{i,j} \\psi_j(x) R \\sum_{j=1}^{N_2} \\hat w_{i,j} \\psi_j(x) dt \\\\\n",
    "\n",
    "\\end{gather}"
   ],
   "id": "d368acf480a655ce"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "aeca6896db411531"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-10T15:27:40.967512Z",
     "start_time": "2025-01-10T15:27:40.730820Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import scipy as sp\n",
    "import control as ct\n",
    "import control.matlab as matlab\n",
    "from HelperFunctions import *\n",
    "from scipy.signal import butter, lfilter\n",
    "\n",
    "%matplotlib notebook"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "f367af89159e2761"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
