{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-29T11:26:59.153787Z",
     "start_time": "2025-05-29T11:26:58.155467Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from LQR_env import LQR_Env\n",
    "from NeuralControllerEst import NeuralControllerEst"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-29T11:26:59.173580Z",
     "start_time": "2025-05-29T11:26:59.160208Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define the LQR environment parameters\n",
    "A = np.array([[1, 1], [0, 1]])\n",
    "B = np.array([[0], [1]])\n",
    "Q = np.eye(2)  # State cost matrix\n",
    "R = np.eye(1)  # Control cost matrix\n",
    "noise_std = 0.1  # Standard deviation of noise\n",
    "\n",
    "# Create the LQR environment\n",
    "env = LQR_Env(A, B, Q, R, noise_std)"
   ],
   "id": "4b41f850719341df",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-29T11:26:59.176791Z",
     "start_time": "2025-05-29T11:26:59.174345Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define the neural controller parameters\n",
    "state_dim = np.shape(A)[0]  # Dimension of the state\n",
    "action_dim = np.shape(B)[1]  # Dimension of the control input\n",
    "value_layers = [16, 8]  # Layers for the value function\n",
    "policy_layers = [16, 8]  # Layers for the policy function\n",
    "learning_rate = 0.1  # Learning rate for the neural network\n",
    "\n",
    "# Create the neural controller\n",
    "LQR_NN = NeuralControllerEst(value_layers, policy_layers, state_dim, action_dim, learning_rate)\n"
   ],
   "id": "641fed19b7f012be",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-29T11:29:03.204458Z",
     "start_time": "2025-05-29T11:27:11.951960Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Train\n",
    "for episode in range(50):\n",
    "    result = LQR_NN.train_episode(env)\n",
    "    if episode % 10 == 0:\n",
    "        print(f\"Episode {episode}: Reward={result['total_reward']:.2f}\")"
   ],
   "id": "fe8a856c6d45bf8f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0: Reward=-1716180978178.27\n",
      "Episode 10: Reward=-281282747368063467520.00\n",
      "Episode 20: Reward=-4525437280201731997696.00\n",
      "Episode 30: Reward=-168510851347969343488.00\n",
      "Episode 40: Reward=-2033165864334.98\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-29T11:32:39.433136Z",
     "start_time": "2025-05-29T11:32:39.383564Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Get the learned policy matrix approximation\n",
    "K_learned, bias, r_squared = LQR_NN.get_policy_matrix_approximation()\n",
    "print(f\"Learned gain matrix: {K_learned}\")\n",
    "print(f\"Linearity score: {r_squared}\") "
   ],
   "id": "baa1da8ee91ff7e7",
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 1 is different from 2)",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mValueError\u001B[39m                                Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[9]\u001B[39m\u001B[32m, line 2\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;66;03m# Get the learned policy matrix approximation\u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m2\u001B[39m K_learned, bias, r_squared = \u001B[43mLQR_NN\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget_policy_matrix_approximation\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m      3\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mLearned gain matrix: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mK_learned\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n\u001B[32m      4\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mLinearity score: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mr_squared\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m) \n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Library/CloudStorage/OneDrive-Personal/University/MsC/Thesis/Theory Codes/LQR_NN/NeuralControllerEst.py:437\u001B[39m, in \u001B[36mNeuralControllerEst.get_policy_matrix_approximation\u001B[39m\u001B[34m(self, state_range, grid_size)\u001B[39m\n\u001B[32m    434\u001B[39m bias = K_with_bias[-\u001B[32m1\u001B[39m]\n\u001B[32m    436\u001B[39m \u001B[38;5;66;03m# Calculate R-squared to see how linear the policy is\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m437\u001B[39m actions_pred = -(\u001B[43mstates\u001B[49m\u001B[43m \u001B[49m\u001B[43m@\u001B[49m\u001B[43m \u001B[49m\u001B[43mK_learned\u001B[49m\u001B[43m.\u001B[49m\u001B[43mT\u001B[49m) + bias\n\u001B[32m    438\u001B[39m ss_res = np.sum((actions - actions_pred) ** \u001B[32m2\u001B[39m)\n\u001B[32m    439\u001B[39m ss_tot = np.sum((actions - np.mean(actions)) ** \u001B[32m2\u001B[39m)\n",
      "\u001B[31mValueError\u001B[39m: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 1 is different from 2)"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-29T11:27:01.778228Z",
     "start_time": "2025-05-29T11:27:01.711852Z"
    }
   },
   "cell_type": "code",
   "source": "LQR_NN.compare_with_optimal_lqr(env)",
   "id": "a46145959e5ddffc",
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 1 is different from 2)",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mValueError\u001B[39m                                Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[5]\u001B[39m\u001B[32m, line 1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m \u001B[43mLQR_NN\u001B[49m\u001B[43m.\u001B[49m\u001B[43mcompare_with_optimal_lqr\u001B[49m\u001B[43m(\u001B[49m\u001B[43menv\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Library/CloudStorage/OneDrive-Personal/University/MsC/Thesis/Theory Codes/LQR_NN/NeuralControllerEst.py:449\u001B[39m, in \u001B[36mNeuralControllerEst.compare_with_optimal_lqr\u001B[39m\u001B[34m(self, env, state_range, num_samples)\u001B[39m\n\u001B[32m    446\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33;03m\"\"\"Compare learned policy with optimal LQR policy.\"\"\"\u001B[39;00m\n\u001B[32m    448\u001B[39m \u001B[38;5;66;03m# Get learned policy approximation\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m449\u001B[39m K_learned, bias, r_squared = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mget_policy_matrix_approximation\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstate_range\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mint\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mnp\u001B[49m\u001B[43m.\u001B[49m\u001B[43msqrt\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnum_samples\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    450\u001B[39m K_optimal = env.K\n\u001B[32m    452\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33m=== Policy Comparison ===\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Library/CloudStorage/OneDrive-Personal/University/MsC/Thesis/Theory Codes/LQR_NN/NeuralControllerEst.py:437\u001B[39m, in \u001B[36mNeuralControllerEst.get_policy_matrix_approximation\u001B[39m\u001B[34m(self, state_range, grid_size)\u001B[39m\n\u001B[32m    434\u001B[39m bias = K_with_bias[-\u001B[32m1\u001B[39m]\n\u001B[32m    436\u001B[39m \u001B[38;5;66;03m# Calculate R-squared to see how linear the policy is\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m437\u001B[39m actions_pred = -(\u001B[43mstates\u001B[49m\u001B[43m \u001B[49m\u001B[43m@\u001B[49m\u001B[43m \u001B[49m\u001B[43mK_learned\u001B[49m\u001B[43m.\u001B[49m\u001B[43mT\u001B[49m) + bias\n\u001B[32m    438\u001B[39m ss_res = np.sum((actions - actions_pred) ** \u001B[32m2\u001B[39m)\n\u001B[32m    439\u001B[39m ss_tot = np.sum((actions - np.mean(actions)) ** \u001B[32m2\u001B[39m)\n",
      "\u001B[31mValueError\u001B[39m: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 1 is different from 2)"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "1cbd27920e5c4866"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
